{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913ca3ab-a4c3-400c-9be1-bc617dd636fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9a4453cf-919d-4cc4-8b22-ad128952b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a962c84e-e1f7-4d33-b5eb-1f9e508fdab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    vocab_size: int = 32000\n",
    "    d_model: int = 768\n",
    "    n_layers: int = 12\n",
    "    n_heads: int = 12\n",
    "    d_head: int = 64\n",
    "    intermediate_size: int = 3072\n",
    "    d_latent: int = 32\n",
    "    d_rope_sub: int = 16\n",
    "    attn_type: Literal['mla', 'mha'] = 'mla' #mla or mha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cfc59ede-d7ff-4e80-9931-2333d5e16ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, features: int, eps: int = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(features))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        norm = (x.pow(2).mean(-1, keepdims=True) + self.eps).sqrt()\n",
    "        out = x / norm\n",
    "        return out * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3e1cca0b-beb1-4131-a714-f5d6474fc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freq_cis(dim: int, seq_len: int, theta: float = 10000.0):\n",
    "    base_freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(seq_len, device=base_freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, base_freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "da965c71-3c88-451c-8482-e696213d7842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor):\n",
    "    x_c = torch.view_as_complex(x.float().reshape(x.shape[:-1] + (x.shape[-1] // 2, 2)))\n",
    "    seq_len = x.shape[2]\n",
    "    freqs_cis = freqs_cis[None, None, :seq_len, :]\n",
    "    x_r = torch.view_as_real(x_b * freq_cis).flatten(3)\n",
    "    return x_r.type_as(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3e4bd4ad-3291-488e-822f-5f053ae0c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.n_heads = config.n_heads\n",
    "        self.d_head = config.d_head\n",
    "        self.d_latent = config.d_latent\n",
    "        self.d_rope_sub = config.d_rope_sub\n",
    "        self.d_nope_sub = self.d_head - self.d_rope_sub\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "\n",
    "        self.kv_latent_norm = RMSNorm(self.d_model)\n",
    "\n",
    "        self.wq = nn.Linear(self.d_model, self.n_heads * self.d_head, bias=True)\n",
    "        self.wkv_a = nn.Linear(self.d_model, self.d_latent, bias=False)\n",
    "        self.wkv_b = nn.Linear(self.d_latent, 2 * self.n_heads * self.d_head, bias=False)\n",
    "        self.out_proj = nn.Linear(self.n_heads * self.d_head, self.d_model, bias=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, past_key_values=None, use_cache=None):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        current_latent_kv = self.kv_latent_norm(self.wkv_a(x))\n",
    "\n",
    "        if past_key_values is None:\n",
    "            cache_latent_kv = current_latent_kv\n",
    "            past_seq_len = 0\n",
    "        else:\n",
    "            cache_latent_kv = torch.cat([past_key_values, current_latent_kv], dim=1)\n",
    "            past_seq_len = past_key_values.shape[1]\n",
    "\n",
    "        q_position_ids = torch.arange(past_seq_len, past_seq_len + seq_len, dtype=torch.long, device=x.device)\n",
    "        k_position_ids = torch.arange(0, past_seq_len + seq_len, dtype=torch.long, device=x.device)\n",
    "        updated_freqs_cis_q = freqs_cis[q_position_ids]\n",
    "        updated_freqs_cis_k = freqs_cis[k_position_ids]\n",
    "\n",
    "        updated_kv_for_layer = cache_latent_kv\n",
    "        kv = self.wkv_b(updated_kv_for_layer)\n",
    "\n",
    "        q = self.wq(x)\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head)\n",
    "        q_nope, q_rope = torch.split(q, [self.d_nope_sub, self.d_rope_sub], dim=-1)\n",
    "        q_roped = apply_rotary_emb(q_rope, updated_freqs_cis_q)\n",
    "        q = torch.cat([q_nope, q_roped], dim=-1)\n",
    "\n",
    "        k, v = torch.chunk(kv, chunks=2, dim=-1)\n",
    "        k = k.view(batch_size, -1, self.n_heads, self.d_head)\n",
    "        k_nope, k_rope = torch.split(k, [self.d_nope_sub, self.d_rope_sub], dim=-1)\n",
    "        k_roped = apply_rotary_emb(k_rope, updated_freqs_cis_k)\n",
    "        k = torch.cat([k_nope, k_roped], dim=-1)\n",
    "\n",
    "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
    "\n",
    "        try:\n",
    "            attn = F.scaled_dot_product_attention(q, k, v, is_causal=True, dropout_p=self.attn_dropout.p if self.training else 0.0)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: F.scaled_dot_product_attention failed ({e}). Falling back to manual attention.\")\n",
    "            T_q = q.shape[-2]\n",
    "            T_k = k.shape[-2]\n",
    "        \n",
    "            scale = 1 / (q.shape[-1] ** 0.5)\n",
    "            attn_weight = q @ k.transpose(-2, -1) * scale\n",
    "            \n",
    "            causal_mask = torch.full((T_q, T_k), float('-inf'), device=q.device, dtype=q.dtype)\n",
    "            mask_ones = torch.ones(T_q, T_k, dtype=torch.bool, device=q.device).tril(diagonal=0)\n",
    "\n",
    "            if T_q == T_k:\n",
    "                attn_weight = attn_weight.masked_fill(mask_ones.logical_not(), float('-inf'))\n",
    "            else:\n",
    "                attn_mask = torch.ones(T_q, T_k, dtype=torch.bool, device=q.device)\n",
    "                for i in range(T_q):\n",
    "                    attn_mask[i, i + past_seq_len + 1:] = 0\n",
    "                attn_weight = attn_weight.masked_fill(attn_mask.logical_not(), float('-inf')) \n",
    "            \n",
    "            attn_weight = F.softmax(attn_weight, dim=-1)\n",
    "            attn_weight = self.attn_dropout(attn_weight)\n",
    "            attn = attn_weight @ v\n",
    "\n",
    "        attn_output = attn.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(batch_size, seq_len, self.n_heads * self.d_head)\n",
    "        \n",
    "        final_output = self.out_proj(attn_output)\n",
    "        final_output = self.attn_dropout(final_output)\n",
    "\n",
    "        if use_cache:\n",
    "            return final_output, updated_kv_for_layer\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8bbac375-57cc-404c-87a6-b8f903b5bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(dim, hidden_dim)\n",
    "        self.l2 = nn.Linear(hidden_dim, dim)\n",
    "        self.l3 = nn.Linear(dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.l2(F.silu(self.l1(x)) * self.l3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "54043b37-5115-4a75-9452-53b0b661dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, config: ModelConfig):\n",
    "        super().__init__()\n",
    "        self.attn = MLA(config)\n",
    "        self.ffn = MLP(config.d_model, config.intermediate_size)\n",
    "        self.attn_norm = RMSNorm(config.d_model)\n",
    "        self.ffn_norm = RMSNorm(config.d_model)\n",
    "        self.attn_dropout = nn.Dropout(config.attn_dropout)\n",
    "        self.ffn_dropout = nn.Dropout(config.ffn_dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, freqs_cis: torch.Tensor, past_key_values=None, use_cache=False):\n",
    "        h = self.attn_norm(x)\n",
    "        attn_output, updated_key_values = self.attn(h, freqs_cis, past_key_values, use_cache)\n",
    "        attn_output = self.attn_dropout(attn_output)\n",
    "        x = x + attn_output\n",
    "        ffn_output = self.ffn_dropout(self.ffn(self.ffn_norm(x)))\n",
    "        x = x + ffn_output\n",
    "\n",
    "        return x, updated_key_values if use_cache else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e8f4a-5327-48f8-b89e-7caa3be7ee9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DaisyLM venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
